{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36fa34e6",
   "metadata": {},
   "source": [
    "# SmartCoins Data Analysis Portfolio\n",
    "\n",
    "**Author:** Bienvenu Mwenyemali  \n",
    "**Data Source:** [SmartCoins App API](https://smartcoinsapp.com/api/coins)  \n",
    "**Skills Demonstrated:** Python, Pandas, API Integration, Data Analysis, Visualization\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "This notebook demonstrates comprehensive data analytics skills using cryptocurrency data from SmartCoins App. We will:\n",
    "1. Extract data from API\n",
    "2. Clean and transform data\n",
    "3. Perform statistical analysis\n",
    "4. Create custom scoring algorithms\n",
    "5. Detect outliers\n",
    "6. Generate visualizations\n",
    "7. Export data for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce053b5a",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c7a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas numpy matplotlib seaborn requests scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289505c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.cm' has no attribute 'register_cmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Try importing seaborn (optional)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     SEABORN_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     sns\u001b[38;5;241m.\u001b[39mset_style(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhitegrid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\seaborn\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmiscplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\seaborn\\matrix.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hierarchy\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cm\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Grid\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (despine, axis_ticklabels_overlap, relative_luminance,\n\u001b[0;32m     16\u001b[0m                     to_utf8)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\seaborn\\cm.py:1582\u001b[0m\n\u001b[0;32m   1579\u001b[0m     _cmap_r \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mListedColormap(_lut[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], _name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_r\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m   1580\u001b[0m     \u001b[38;5;28mlocals\u001b[39m()[_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_r\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _cmap_r\n\u001b[1;32m-> 1582\u001b[0m     \u001b[43mmpl_cm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_cmap\u001b[49m(_name, _cmap)\n\u001b[0;32m   1583\u001b[0m     mpl_cm\u001b[38;5;241m.\u001b[39mregister_cmap(_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_r\u001b[39m\u001b[38;5;124m\"\u001b[39m, _cmap_r)\n\u001b[0;32m   1585\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m colors, mpl_cm\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.cm' has no attribute 'register_cmap'"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c63e154",
   "metadata": {},
   "source": [
    "## Section 2: Data Extraction from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f11bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "API_URL = \"https://smartcoinsapp.com/api/coins\"\n",
    "\n",
    "def extract_from_api(url):\n",
    "    \"\"\"Extract cryptocurrency data from SmartCoins API\"\"\"\n",
    "    print(f\"Fetching data from: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            print(f\"Successfully extracted {len(data)} coins\")\n",
    "            return data\n",
    "        elif isinstance(data, dict):\n",
    "            coins = data.get('data', data.get('coins', [data]))\n",
    "            print(f\"Successfully extracted {len(coins)} coins\")\n",
    "            return coins\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract data\n",
    "raw_data = extract_from_api(API_URL)\n",
    "print(f\"\\nTotal coins retrieved: {len(raw_data) if raw_data else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sample of raw data structure\n",
    "if raw_data:\n",
    "    print(\"Sample coin data structure:\")\n",
    "    print(json.dumps(raw_data[0], indent=2)[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df10b1",
   "metadata": {},
   "source": [
    "## Section 3: Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataframe(coins_data):\n",
    "    \"\"\"Transform raw API data into a structured DataFrame\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for coin in coins_data:\n",
    "        # Extract nested data\n",
    "        quote = coin.get('quote', {}).get('USD', {})\n",
    "        signals = coin.get('signals', {})\n",
    "        scores = coin.get('investmentScores', {})\n",
    "        risk = coin.get('riskMetrics', {})\n",
    "        network = coin.get('networkMetrics', {})\n",
    "        supply = coin.get('supplyMetrics', {})\n",
    "        \n",
    "        record = {\n",
    "            # Basic Info\n",
    "            'coin_name': coin.get('name'),\n",
    "            'symbol': coin.get('symbol'),\n",
    "            'price_usd': quote.get('price', 0),\n",
    "            'market_cap': quote.get('market_cap', 0),\n",
    "            'volume_24h': quote.get('volume_24h', 0),\n",
    "            'volume_change_24h': quote.get('volume_change_24h', 0),\n",
    "            \n",
    "            # Price Changes\n",
    "            'pct_change_1h': quote.get('percent_change_1h', 0),\n",
    "            'pct_change_24h': quote.get('percent_change_24h', 0),\n",
    "            'pct_change_7d': quote.get('percent_change_7d', 0),\n",
    "            'pct_change_30d': quote.get('percent_change_30d', 0),\n",
    "            'pct_change_60d': quote.get('percent_change_60d', 0),\n",
    "            'pct_change_90d': quote.get('percent_change_90d', 0),\n",
    "            \n",
    "            # Classification\n",
    "            'coin_type': coin.get('coinType', 'Unknown'),\n",
    "            'platform': coin.get('platform', {}).get('name', 'Native') if coin.get('platform') else 'Native',\n",
    "            'category': coin.get('category', 'Uncategorized'),\n",
    "            \n",
    "            # Signals\n",
    "            'primary_signal': signals.get('primarySignal', 'NEUTRAL'),\n",
    "            'signal_strength': signals.get('signalStrength', 0),\n",
    "            'overall_score': signals.get('overallScore', 0),\n",
    "            'composite_score': signals.get('compositeScore', 0),\n",
    "            \n",
    "            # Momentum\n",
    "            'change_momentum': signals.get('changeMomentum', 0),\n",
    "            'momentum_acceleration': signals.get('momentumAcceleration', 0),\n",
    "            'risk_adjusted_momentum': signals.get('riskAdjustedMomentum', 0),\n",
    "            \n",
    "            # Risk Metrics\n",
    "            'price_volatility': risk.get('priceVolatility', 0),\n",
    "            'volatility_risk': risk.get('volatilityRisk', 0),\n",
    "            'liquidity_risk': risk.get('liquidityRisk', 0),\n",
    "            'concentration_risk': risk.get('concentrationRisk', 0),\n",
    "            \n",
    "            # Network Metrics\n",
    "            'nvt_score': network.get('nvtScore', 0),\n",
    "            'mvrv_score': network.get('mvrvScore', 0),\n",
    "            'scarcity_score': network.get('scarcityScore', 0),\n",
    "            'efficiency_score': network.get('efficiencyScore', 0),\n",
    "            'momentum_consistency': network.get('momentumConsistency', 0),\n",
    "            \n",
    "            # Investment Scores\n",
    "            'inv_momentum_score': scores.get('momentumScore', 0),\n",
    "            'inv_value_score': scores.get('valueScore', 0),\n",
    "            'inv_risk_score': scores.get('riskScore', 0),\n",
    "            'inv_activity_score': scores.get('activityScore', 0),\n",
    "            'inv_network_score': scores.get('networkScore', 0),\n",
    "            \n",
    "            # Dates\n",
    "            'date_added': coin.get('date_added'),\n",
    "            'last_updated': coin.get('last_updated'),\n",
    "            \n",
    "            # Supply Metrics\n",
    "            'max_supply': coin.get('max_supply'),\n",
    "            'circulating_supply': coin.get('circulating_supply', 0),\n",
    "            'total_supply': coin.get('total_supply', 0),\n",
    "            'annual_inflation': supply.get('annualInflation', 0),\n",
    "            'stock_to_flow': supply.get('stockToFlow', 0),\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Create DataFrame\n",
    "df = transform_to_dataframe(raw_data)\n",
    "print(f\"DataFrame created with {len(df)} rows and {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128390f",
   "metadata": {},
   "source": [
    "## Section 4: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Shape and Info\n",
    "print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Types\n",
    "print(\"Data Types:\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6439308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb605711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values Analysis\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "})\n",
    "print(\"Missing Values:\")\n",
    "missing_df[missing_df['Missing Count'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f10f92",
   "metadata": {},
   "source": [
    "## Section 5: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb11073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Fill missing values\n",
    "df_clean['max_supply'] = df_clean['max_supply'].fillna(0)\n",
    "print(\"Filled missing max_supply values with 0\")\n",
    "\n",
    "# Convert date columns\n",
    "for col in ['date_added', 'last_updated']:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "        print(f\"Converted {col} to datetime\")\n",
    "\n",
    "# Remove duplicates\n",
    "initial_rows = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates(subset=['symbol'], keep='first')\n",
    "print(f\"Removed {initial_rows - len(df_clean)} duplicate records\")\n",
    "\n",
    "print(f\"\\nCleaned DataFrame: {len(df_clean)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0860aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Derived Features\n",
    "\n",
    "# Price Tier Classification\n",
    "def classify_price(price):\n",
    "    if price < 0.001:\n",
    "        return 'Micro'\n",
    "    elif price < 1:\n",
    "        return 'Low'\n",
    "    elif price < 100:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "df_clean['price_tier'] = df_clean['price_usd'].apply(classify_price)\n",
    "print(\"Created price_tier classification\")\n",
    "\n",
    "# Momentum Category\n",
    "def classify_momentum(momentum):\n",
    "    if momentum > 1:\n",
    "        return 'Strong Bullish'\n",
    "    elif momentum > 0:\n",
    "        return 'Bullish'\n",
    "    elif momentum > -1:\n",
    "        return 'Bearish'\n",
    "    else:\n",
    "        return 'Strong Bearish'\n",
    "\n",
    "df_clean['momentum_category'] = df_clean['change_momentum'].apply(classify_momentum)\n",
    "print(\"Created momentum_category classification\")\n",
    "\n",
    "# Risk Level Classification\n",
    "def classify_risk(volatility_risk):\n",
    "    if volatility_risk < 0.5:\n",
    "        return 'Low Risk'\n",
    "    elif volatility_risk < 2:\n",
    "        return 'Medium Risk'\n",
    "    else:\n",
    "        return 'High Risk'\n",
    "\n",
    "df_clean['risk_level'] = df_clean['volatility_risk'].apply(classify_risk)\n",
    "print(\"Created risk_level classification\")\n",
    "\n",
    "print(f\"\\nNew columns: price_tier, momentum_category, risk_level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8faa58",
   "metadata": {},
   "source": [
    "## Section 6: Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd60690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics for Key Metrics\n",
    "key_metrics = ['price_usd', 'volume_24h', 'overall_score', 'composite_score', \n",
    "               'change_momentum', 'price_volatility', 'volatility_risk']\n",
    "\n",
    "available_metrics = [m for m in key_metrics if m in df_clean.columns]\n",
    "df_clean[available_metrics].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1444ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "score_columns = ['overall_score', 'composite_score', 'change_momentum', \n",
    "                 'price_volatility', 'efficiency_score', 'inv_momentum_score']\n",
    "available_cols = [c for c in score_columns if c in df_clean.columns]\n",
    "\n",
    "corr_matrix = df_clean[available_cols].corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "corr_matrix.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17452a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Correlations\n",
    "corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_pairs.append({\n",
    "            'Pair': f\"{corr_matrix.columns[i]} <-> {corr_matrix.columns[j]}\",\n",
    "            'Correlation': abs(corr_matrix.iloc[i, j])\n",
    "        })\n",
    "\n",
    "corr_df = pd.DataFrame(corr_pairs).sort_values('Correlation', ascending=False)\n",
    "print(\"Top 10 Strongest Correlations:\")\n",
    "corr_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca11c77c",
   "metadata": {},
   "source": [
    "## Section 7: Custom Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99460cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_momentum_score(row):\n",
    "    \"\"\"Calculate custom momentum score based on multiple timeframe changes\"\"\"\n",
    "    weights = {\n",
    "        'pct_change_1h': 0.05,\n",
    "        'pct_change_24h': 0.15,\n",
    "        'pct_change_7d': 0.25,\n",
    "        'pct_change_30d': 0.30,\n",
    "        'pct_change_90d': 0.25\n",
    "    }\n",
    "    \n",
    "    score = 0\n",
    "    for col, weight in weights.items():\n",
    "        if col in row.index and pd.notna(row[col]):\n",
    "            normalized = np.tanh(row[col] / 50) * 50\n",
    "            score += normalized * weight\n",
    "    \n",
    "    return round(score, 2)\n",
    "\n",
    "def calculate_risk_score(row):\n",
    "    \"\"\"Calculate custom risk score (lower is better)\"\"\"\n",
    "    risk_factors = [\n",
    "        ('volatility_risk', 0.35),\n",
    "        ('liquidity_risk', 0.30),\n",
    "        ('concentration_risk', 0.20),\n",
    "        ('price_volatility', 0.15)\n",
    "    ]\n",
    "    \n",
    "    score = 0\n",
    "    for col, weight in risk_factors:\n",
    "        if col in row.index and pd.notna(row[col]):\n",
    "            normalized = min(row[col] / 10, 10)\n",
    "            score += normalized * weight\n",
    "    \n",
    "    return round(score, 2)\n",
    "\n",
    "def calculate_investment_score(row):\n",
    "    \"\"\"Calculate overall investment score\"\"\"\n",
    "    momentum = row.get('momentum_score', 0)\n",
    "    risk = row.get('risk_score', 5)\n",
    "    overall = row.get('overall_score', 50)\n",
    "    \n",
    "    # Normalize components\n",
    "    momentum_normalized = (momentum + 50) / 100 * 40\n",
    "    risk_normalized = (10 - min(risk, 10)) / 10 * 30\n",
    "    overall_normalized = overall / 100 * 30\n",
    "    \n",
    "    return round(momentum_normalized + risk_normalized + overall_normalized, 2)\n",
    "\n",
    "# Apply scoring functions\n",
    "df_clean['momentum_score'] = df_clean.apply(calculate_momentum_score, axis=1)\n",
    "df_clean['risk_score'] = df_clean.apply(calculate_risk_score, axis=1)\n",
    "df_clean['investment_score'] = df_clean.apply(calculate_investment_score, axis=1)\n",
    "\n",
    "print(\"Custom scores calculated:\")\n",
    "print(\"- momentum_score: Based on multi-timeframe price changes\")\n",
    "print(\"- risk_score: Based on volatility and liquidity risks\")\n",
    "print(\"- investment_score: Composite score for investment decisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bcc0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal Prediction Function\n",
    "def predict_signal(row):\n",
    "    \"\"\"Predict trading signal based on multiple factors\"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Momentum contribution\n",
    "    if row.get('momentum_score', 0) > 20:\n",
    "        score += 2\n",
    "    elif row.get('momentum_score', 0) > 0:\n",
    "        score += 1\n",
    "    elif row.get('momentum_score', 0) < -20:\n",
    "        score -= 2\n",
    "    elif row.get('momentum_score', 0) < 0:\n",
    "        score -= 1\n",
    "    \n",
    "    # Risk contribution\n",
    "    if row.get('risk_score', 5) < 2:\n",
    "        score += 1\n",
    "    elif row.get('risk_score', 5) > 5:\n",
    "        score -= 1\n",
    "    \n",
    "    # Investment score contribution\n",
    "    if row.get('investment_score', 50) > 60:\n",
    "        score += 1\n",
    "    elif row.get('investment_score', 50) < 40:\n",
    "        score -= 1\n",
    "    \n",
    "    # Determine signal\n",
    "    if score >= 3:\n",
    "        return 'STRONG BUY'\n",
    "    elif score >= 1:\n",
    "        return 'BUY'\n",
    "    elif score <= -3:\n",
    "        return 'STRONG SELL'\n",
    "    elif score <= -1:\n",
    "        return 'SELL'\n",
    "    else:\n",
    "        return 'HOLD'\n",
    "\n",
    "df_clean['predicted_signal'] = df_clean.apply(predict_signal, axis=1)\n",
    "\n",
    "print(\"Signal Distribution:\")\n",
    "df_clean['predicted_signal'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd7058",
   "metadata": {},
   "source": [
    "## Section 8: Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(df, column, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "    if column not in df.columns:\n",
    "        return pd.Series([False] * len(df))\n",
    "    \n",
    "    z_scores = np.abs(stats.zscore(df[column].fillna(0)))\n",
    "    return z_scores > threshold\n",
    "\n",
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    if column not in df.columns:\n",
    "        return pd.Series([False] * len(df))\n",
    "    \n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    return (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "\n",
    "# Detect outliers in key columns\n",
    "outlier_columns = ['price_usd', 'volume_24h', 'price_volatility', 'investment_score']\n",
    "\n",
    "print(\"Outlier Detection Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for col in outlier_columns:\n",
    "    if col in df_clean.columns:\n",
    "        zscore_outliers = detect_outliers_zscore(df_clean, col)\n",
    "        iqr_outliers = detect_outliers_iqr(df_clean, col)\n",
    "        \n",
    "        df_clean[f'{col}_outlier_zscore'] = zscore_outliers\n",
    "        df_clean[f'{col}_outlier_iqr'] = iqr_outliers\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Z-score outliers: {zscore_outliers.sum()}\")\n",
    "        print(f\"  IQR outliers: {iqr_outliers.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c9335",
   "metadata": {},
   "source": [
    "## Section 9: Top N Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 by Investment Score\n",
    "top_investment = df_clean.nlargest(10, 'investment_score')[[\n",
    "    'coin_name', 'symbol', 'investment_score', 'momentum_score', \n",
    "    'risk_score', 'predicted_signal'\n",
    "]]\n",
    "print(\"TOP 10 COINS BY INVESTMENT SCORE\")\n",
    "print(\"=\" * 60)\n",
    "top_investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Lowest Risk\n",
    "top_low_risk = df_clean.nsmallest(10, 'risk_score')[[\n",
    "    'coin_name', 'symbol', 'risk_score', 'risk_level'\n",
    "]]\n",
    "print(\"TOP 10 LOWEST RISK COINS\")\n",
    "print(\"=\" * 60)\n",
    "top_low_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 by Momentum\n",
    "top_momentum = df_clean.nlargest(10, 'momentum_score')[[\n",
    "    'coin_name', 'symbol', 'momentum_score', 'change_momentum'\n",
    "]]\n",
    "print(\"TOP 10 COINS BY MOMENTUM\")\n",
    "print(\"=\" * 60)\n",
    "top_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Analysis\n",
    "print(\"DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nPrice Tier Distribution:\")\n",
    "print(df_clean['price_tier'].value_counts())\n",
    "\n",
    "print(\"\\nRisk Level Distribution:\")\n",
    "print(df_clean['risk_level'].value_counts())\n",
    "\n",
    "print(\"\\nSignal Distribution:\")\n",
    "print(df_clean['predicted_signal'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc90782d",
   "metadata": {},
   "source": [
    "## Section 10: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdd552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Score Distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Investment Score\n",
    "axes[0, 0].hist(df_clean['investment_score'].dropna(), bins=20, color='steelblue', edgecolor='white')\n",
    "axes[0, 0].set_title('Investment Score Distribution', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Investment Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(df_clean['investment_score'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Momentum Score\n",
    "axes[0, 1].hist(df_clean['momentum_score'].dropna(), bins=20, color='forestgreen', edgecolor='white')\n",
    "axes[0, 1].set_title('Momentum Score Distribution', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Momentum Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].axvline(df_clean['momentum_score'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Risk Score\n",
    "axes[1, 0].hist(df_clean['risk_score'].dropna(), bins=20, color='coral', edgecolor='white')\n",
    "axes[1, 0].set_title('Risk Score Distribution', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Risk Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].axvline(df_clean['risk_score'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Overall Score\n",
    "axes[1, 1].hist(df_clean['overall_score'].dropna(), bins=20, color='purple', edgecolor='white')\n",
    "axes[1, 1].set_title('Overall Score Distribution', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Overall Score')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].axvline(df_clean['overall_score'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Scatter Plot for Outlier Detection\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Investment vs Risk Score\n",
    "colors = ['red' if x else 'steelblue' for x in df_clean.get('investment_score_outlier_iqr', [False]*len(df_clean))]\n",
    "axes[0].scatter(df_clean['risk_score'], df_clean['investment_score'], \n",
    "                c=colors, alpha=0.6, s=50)\n",
    "axes[0].set_xlabel('Risk Score', fontsize=12)\n",
    "axes[0].set_ylabel('Investment Score', fontsize=12)\n",
    "axes[0].set_title('Investment Score vs Risk Score\\n(Red = Outliers)', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume vs Price (log scale)\n",
    "df_positive = df_clean[(df_clean['volume_24h'] > 0) & (df_clean['price_usd'] > 0)]\n",
    "colors2 = ['red' if x else 'forestgreen' for x in df_positive.get('volume_24h_outlier_iqr', [False]*len(df_positive))]\n",
    "axes[1].scatter(np.log10(df_positive['price_usd'] + 1), \n",
    "                np.log10(df_positive['volume_24h'] + 1),\n",
    "                c=colors2, alpha=0.6, s=50)\n",
    "axes[1].set_xlabel('Log10(Price USD)', fontsize=12)\n",
    "axes[1].set_ylabel('Log10(Volume 24h)', fontsize=12)\n",
    "axes[1].set_title('Volume vs Price (Log Scale)\\n(Red = Outliers)', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ca27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Top 10 Coins Bar Chart\n",
    "top10 = df_clean.nlargest(10, 'investment_score')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = range(len(top10))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar([i - width/2 for i in x], top10['investment_score'], width, \n",
    "               label='Investment Score', color='steelblue')\n",
    "bars2 = ax.bar([i + width/2 for i in x], top10['momentum_score'], width,\n",
    "               label='Momentum Score', color='coral')\n",
    "\n",
    "ax.set_xlabel('Coin', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Top 10 Coins: Investment vs Momentum Scores', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(top10['symbol'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Correlation Heatmap\n",
    "score_cols = ['investment_score', 'momentum_score', 'risk_score', 'overall_score',\n",
    "              'composite_score', 'change_momentum', 'price_volatility']\n",
    "available = [c for c in score_cols if c in df_clean.columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = df_clean[available].corr()\n",
    "\n",
    "# Create heatmap using matplotlib\n",
    "im = ax.imshow(corr, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_xticks(range(len(available)))\n",
    "ax.set_yticks(range(len(available)))\n",
    "ax.set_xticklabels(available, rotation=45, ha='right')\n",
    "ax.set_yticklabels(available)\n",
    "\n",
    "# Add correlation values as text\n",
    "for i in range(len(available)):\n",
    "    for j in range(len(available)):\n",
    "        ax.text(j, i, f'{corr.iloc[i, j]:.2f}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "ax.set_title('Correlation Heatmap of Key Metrics', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Risk Level Pie Chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Risk Level\n",
    "risk_counts = df_clean['risk_level'].value_counts()\n",
    "colors1 = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "axes[0].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%',\n",
    "            colors=colors1, startangle=90)\n",
    "axes[0].set_title('Risk Level Distribution', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Signal Distribution\n",
    "signal_counts = df_clean['predicted_signal'].value_counts()\n",
    "colors2 = ['#27ae60', '#2ecc71', '#f39c12', '#e74c3c', '#c0392b']\n",
    "axes[1].pie(signal_counts.values, labels=signal_counts.index, autopct='%1.1f%%',\n",
    "            colors=colors2[:len(signal_counts)], startangle=90)\n",
    "axes[1].set_title('Predicted Signal Distribution', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Box Plots for Score Comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Investment Score by Risk Level\n",
    "data_inv = [df_clean[df_clean['risk_level'] == level]['investment_score'].dropna() \n",
    "            for level in ['Low Risk', 'Medium Risk', 'High Risk']]\n",
    "bp1 = axes[0].boxplot([d for d in data_inv if len(d) > 0], patch_artist=True)\n",
    "axes[0].set_xticklabels(['Low Risk', 'Medium Risk', 'High Risk'])\n",
    "axes[0].set_ylabel('Investment Score')\n",
    "axes[0].set_title('Investment Score by Risk Level', fontweight='bold')\n",
    "for patch, color in zip(bp1['boxes'], ['#2ecc71', '#f39c12', '#e74c3c']):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Momentum Score by Price Tier\n",
    "data_mom = [df_clean[df_clean['price_tier'] == tier]['momentum_score'].dropna()\n",
    "            for tier in ['Micro', 'Low', 'Medium', 'High']]\n",
    "bp2 = axes[1].boxplot([d for d in data_mom if len(d) > 0], patch_artist=True)\n",
    "axes[1].set_xticklabels(['Micro', 'Low', 'Medium', 'High'])\n",
    "axes[1].set_ylabel('Momentum Score')\n",
    "axes[1].set_title('Momentum Score by Price Tier', fontweight='bold')\n",
    "for patch, color in zip(bp2['boxes'], ['#3498db', '#9b59b6', '#e67e22', '#1abc9c']):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Risk Score by Signal\n",
    "signals = df_clean['predicted_signal'].unique()\n",
    "data_risk = [df_clean[df_clean['predicted_signal'] == sig]['risk_score'].dropna() for sig in signals]\n",
    "bp3 = axes[2].boxplot([d for d in data_risk if len(d) > 0], patch_artist=True)\n",
    "axes[2].set_xticklabels(signals, rotation=45, ha='right')\n",
    "axes[2].set_ylabel('Risk Score')\n",
    "axes[2].set_title('Risk Score by Signal', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a986bb0a",
   "metadata": {},
   "source": [
    "## Section 11: Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5973549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "os.makedirs('output/data', exist_ok=True)\n",
    "os.makedirs('output/reports', exist_ok=True)\n",
    "\n",
    "# Export to CSV\n",
    "df_clean.to_csv('output/data/smartcoins_analyzed.csv', index=False)\n",
    "print(\"Exported: output/data/smartcoins_analyzed.csv\")\n",
    "\n",
    "# Export Top 50\n",
    "top50 = df_clean.nlargest(50, 'investment_score')\n",
    "top50.to_csv('output/data/top_coins.csv', index=False)\n",
    "print(\"Exported: output/data/top_coins.csv\")\n",
    "\n",
    "# Export to SQLite for SQL analysis\n",
    "conn = sqlite3.connect('output/data/smartcoins.db')\n",
    "df_clean.to_sql('coins', conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "print(\"Exported: output/data/smartcoins.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f23ff3",
   "metadata": {},
   "source": [
    "## Section 12: Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SMARTCOINS ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nTotal Coins Analyzed: {len(df_clean)}\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"KEY STATISTICS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Average Investment Score: {df_clean['investment_score'].mean():.2f}\")\n",
    "print(f\"Average Momentum Score: {df_clean['momentum_score'].mean():.2f}\")\n",
    "print(f\"Average Risk Score: {df_clean['risk_score'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"TOP 5 INVESTMENT OPPORTUNITIES\")\n",
    "print(\"-\" * 50)\n",
    "top5 = df_clean.nlargest(5, 'investment_score')[['coin_name', 'symbol', 'investment_score', 'predicted_signal']]\n",
    "for i, row in top5.iterrows():\n",
    "    print(f\"  {row['symbol']:10s} - Score: {row['investment_score']:6.2f} - Signal: {row['predicted_signal']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"SIGNAL DISTRIBUTION\")\n",
    "print(\"-\" * 50)\n",
    "for signal, count in df_clean['predicted_signal'].value_counts().items():\n",
    "    print(f\"  {signal:15s}: {count:3d} ({count/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"SKILLS DEMONSTRATED\")\n",
    "print(\"-\" * 50)\n",
    "skills = [\n",
    "    \"API Data Extraction (requests)\",\n",
    "    \"Data Transformation (pandas)\",\n",
    "    \"Data Cleaning and Preprocessing\",\n",
    "    \"Statistical Analysis (scipy.stats)\",\n",
    "    \"Custom Scoring Functions\",\n",
    "    \"Outlier Detection (Z-score, IQR)\",\n",
    "    \"Data Visualization (matplotlib)\",\n",
    "    \"SQL Database Export (sqlite3)\"\n",
    "]\n",
    "for skill in skills:\n",
    "    print(f\"  - {skill}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
